{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d34bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'gender' column: ['female' 'bot' 'male']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "# print unique values from the 'gender' column\n",
    "if 'gender' in df.columns:\n",
    "    unique_genders = df['gender'].unique()\n",
    "    print(\"Unique values in 'gender' column:\", unique_genders)\n",
    "else:\n",
    "    print(\"'gender' column not found in the DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65448928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression ===\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[394  10   8]\n",
      " [ 10 175  21]\n",
      " [  7  26 173]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.96      0.96      0.96       412\n",
      "      female       0.83      0.85      0.84       206\n",
      "        male       0.86      0.84      0.85       206\n",
      "\n",
      "    accuracy                           0.90       824\n",
      "   macro avg       0.88      0.88      0.88       824\n",
      "weighted avg       0.90      0.90      0.90       824\n",
      "\n",
      "Validation Accuracy: 0.9\n",
      "\n",
      "=== Random Forest ===\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[379  15  18]\n",
      " [ 14 166  26]\n",
      " [ 13  42 151]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.93      0.92      0.93       412\n",
      "      female       0.74      0.81      0.77       206\n",
      "        male       0.77      0.73      0.75       206\n",
      "\n",
      "    accuracy                           0.84       824\n",
      "   macro avg       0.82      0.82      0.82       824\n",
      "weighted avg       0.85      0.84      0.85       824\n",
      "\n",
      "Validation Accuracy: 0.84\n",
      "\n",
      "=== Gradient Boosting Classifier ===\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[388  11  13]\n",
      " [  9 166  31]\n",
      " [ 11  31 164]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.95      0.94      0.95       412\n",
      "      female       0.80      0.81      0.80       206\n",
      "        male       0.79      0.80      0.79       206\n",
      "\n",
      "    accuracy                           0.87       824\n",
      "   macro avg       0.85      0.85      0.85       824\n",
      "weighted avg       0.87      0.87      0.87       824\n",
      "\n",
      "Validation Accuracy: 0.87\n",
      "\n",
      "=== SVM (Linear Kernel) ===\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[397   7   8]\n",
      " [ 10 178  18]\n",
      " [  4  26 176]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.97      0.96      0.96       412\n",
      "      female       0.84      0.86      0.85       206\n",
      "        male       0.87      0.85      0.86       206\n",
      "\n",
      "    accuracy                           0.91       824\n",
      "   macro avg       0.89      0.89      0.89       824\n",
      "weighted avg       0.91      0.91      0.91       824\n",
      "\n",
      "Validation Accuracy: 0.91\n",
      "\n",
      "=== Test Results for Logistic Regression ===\n",
      "Confusion Matrix:\n",
      " [[1167   44  109]\n",
      " [  35  498  127]\n",
      " [  73  158  429]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.92      0.88      0.90      1320\n",
      "      female       0.71      0.75      0.73       660\n",
      "        male       0.65      0.65      0.65       660\n",
      "\n",
      "    accuracy                           0.79      2640\n",
      "   macro avg       0.76      0.76      0.76      2640\n",
      "weighted avg       0.80      0.79      0.79      2640\n",
      "\n",
      "Test Accuracy: 0.79\n",
      "\n",
      "=== Test Results for Random Forest ===\n",
      "Confusion Matrix:\n",
      " [[1143   66  111]\n",
      " [  32  484  144]\n",
      " [  78  173  409]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.91      0.87      0.89      1320\n",
      "      female       0.67      0.73      0.70       660\n",
      "        male       0.62      0.62      0.62       660\n",
      "\n",
      "    accuracy                           0.77      2640\n",
      "   macro avg       0.73      0.74      0.74      2640\n",
      "weighted avg       0.78      0.77      0.77      2640\n",
      "\n",
      "Test Accuracy: 0.77\n",
      "\n",
      "=== Test Results for Gradient Boosting Classifier ===\n",
      "Confusion Matrix:\n",
      " [[1154   65  101]\n",
      " [  41  488  131]\n",
      " [  74  147  439]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.91      0.87      0.89      1320\n",
      "      female       0.70      0.74      0.72       660\n",
      "        male       0.65      0.67      0.66       660\n",
      "\n",
      "    accuracy                           0.79      2640\n",
      "   macro avg       0.75      0.76      0.76      2640\n",
      "weighted avg       0.79      0.79      0.79      2640\n",
      "\n",
      "Test Accuracy: 0.79\n",
      "\n",
      "=== Test Results for SVM (Linear Kernel) ===\n",
      "Confusion Matrix:\n",
      " [[1134   47  139]\n",
      " [  36  503  121]\n",
      " [  60  178  422]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.92      0.86      0.89      1320\n",
      "      female       0.69      0.76      0.72       660\n",
      "        male       0.62      0.64      0.63       660\n",
      "\n",
      "    accuracy                           0.78      2640\n",
      "   macro avg       0.74      0.75      0.75      2640\n",
      "weighted avg       0.79      0.78      0.78      2640\n",
      "\n",
      "Test Accuracy: 0.78\n",
      "\n",
      "\n",
      "=== Top Features for Random Forest ===\n",
      "     Feature  Importance\n",
      "872    thank    0.023627\n",
      "467     just    0.020552\n",
      "873   thanks    0.017114\n",
      "890  tonight    0.016945\n",
      "959     week    0.016559\n",
      "887    today    0.014055\n",
      "994     year    0.012740\n",
      "503     like    0.011481\n",
      "877    think    0.009786\n",
      "512       ll    0.009752\n",
      "\n",
      "=== Top Features for Gradient Boosting Classifier ===\n",
      "          Feature  Importance\n",
      "887         today    0.124540\n",
      "873        thanks    0.091698\n",
      "872         thank    0.082680\n",
      "467          just    0.060194\n",
      "331          fuck    0.024406\n",
      "501          life    0.021094\n",
      "973         women    0.020763\n",
      "890       tonight    0.020269\n",
      "450  introduction    0.018903\n",
      "615           omg    0.017477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# load the dataset and replace missing values in the 'tweet' column\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty strings\n",
    "\n",
    "# extract tweet text and gender labels\n",
    "tweets = df['tweet']\n",
    "gender = df['gender']\n",
    "\n",
    "# convert tweets to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "y = gender\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)\n",
    "\n",
    "# define classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=39, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=39, n_estimators=100),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(random_state=39),\n",
    "    \"SVM (Linear Kernel)\": SVC(random_state=39, kernel='linear', probability=True)\n",
    "}\n",
    "\n",
    "# train and evaluate each classifier\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"=== {name} ===\")\n",
    "    # train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # evaluate on validation data\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    accuracy_val = round(accuracy_score(y_val, y_pred_val), 2)\n",
    "    \n",
    "    print(\"Validation Results:\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_val))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_val, y_pred_val))\n",
    "    print(\"Validation Accuracy:\", accuracy_val)\n",
    "    print()\n",
    "    \n",
    "    # store results\n",
    "    results[name] = {\n",
    "        \"model\": clf,\n",
    "        \"accuracy\": accuracy_val\n",
    "    }\n",
    "\n",
    "# load the test dataset and process similarly\n",
    "df_test = pd.read_csv('removed_test.csv')\n",
    "df_test['tweet'] = df_test['tweet'].fillna('')  # replace NaN with empty strings\n",
    "\n",
    "# extract test tweets and gender\n",
    "tweets_test = df_test['tweet']\n",
    "gender_test = df_test['gender']\n",
    "\n",
    "# convert test tweets to TF-IDF features\n",
    "X_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# predict and evaluate each classifier on test data\n",
    "for name, metrics in results.items():\n",
    "    clf = metrics[\"model\"]\n",
    "    print(f\"=== Test Results for {name} ===\")\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    accuracy_test = round(accuracy_score(gender_test, y_test_pred), 2)\n",
    "    \n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(gender_test, y_test_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(gender_test, y_test_pred))\n",
    "    print(\"Test Accuracy:\", accuracy_test)\n",
    "    print()\n",
    "\n",
    "# feature importance analysis for Tree-Based Models\n",
    "for name in [\"Random Forest\", \"Gradient Boosting Classifier\"]:\n",
    "    if name in results:\n",
    "        clf = results[name][\"model\"]\n",
    "        feature_importances = clf.feature_importances_\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # create a DataFrame for feature importances\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importances\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n=== Top Features for {name} ===\")\n",
    "        print(importance_df.head(10))\n",
    "        importance_df.to_csv(f'top_features_{name.replace(\" \", \"_\").lower()}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2becf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features for class 'bot':\n",
      "small: 1.5534565107828098\n",
      "introduction: 1.5258475559051394\n",
      "cnn: 1.1508878805796838\n",
      "born: 1.1073603695772705\n",
      "hea: 1.102907741668665\n",
      "history: 1.0832427842507042\n",
      "world: 1.0321138321919079\n",
      "eyes: 0.9712056960095425\n",
      "developer: 0.9617263501346872\n",
      "york: 0.9542586113081575\n",
      "\n",
      "Top 10 features for class 'female':\n",
      "women: 3.0176813503397155\n",
      "thank: 3.012383406331091\n",
      "xx: 2.0645947984007993\n",
      "omg: 1.9792032203179168\n",
      "video: 1.8864329567324674\n",
      "instagram: 1.851036238295689\n",
      "rights: 1.842833811906392\n",
      "just: 1.7808719429894717\n",
      "social: 1.6206442007798707\n",
      "day: 1.5474327011476339\n",
      "\n",
      "Top 10 features for class 'male':\n",
      "iot: 2.5061171508761424\n",
      "mate: 2.422898514342831\n",
      "good: 2.1347021715107317\n",
      "city: 1.7329655532521164\n",
      "nice: 1.6545965520248\n",
      "thanks: 1.583224136456463\n",
      "dc: 1.5163200087354887\n",
      "fans: 1.4110436868136529\n",
      "new: 1.4049069399912817\n",
      "beer: 1.4020865119977892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get feature names and coefficients\n",
    "log_reg_model = results[\"Logistic Regression\"][\"model\"]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = log_reg_model.coef_ \n",
    "classes = log_reg_model.classes_\n",
    "\n",
    "# find top features for each class\n",
    "n_top_features = 10\n",
    "for idx, label in enumerate(classes):\n",
    "    print(f\"Top {n_top_features} features for class '{label}':\")\n",
    "    top_indices = np.argsort(coefficients[idx])[-n_top_features:][::-1]  # descending order\n",
    "    top_features = [(feature_names[i], coefficients[idx][i]) for i in top_indices]\n",
    "    for feature, coef in top_features:\n",
    "        print(f\"{feature}: {coef}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
