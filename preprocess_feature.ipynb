{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b623f30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  label  gender\n",
      "0  alex is too nice for love island :( teenager c...  human  female\n",
      "1  the crypto finance ecosystem by women are not ...  human  female\n",
      "2  check out these awesome cooking t-shi s &amp; ...    bot     bot\n",
      "3  yewwinfo tiny nanopa icles to treat a huge pro...    bot     bot\n",
      "4  sr. project manager water / wastewater enginee...    bot     bot\n",
      "                                               tweet  label  gender\n",
      "0  her attitude was: if you're on her mom. this t...    bot     bot\n",
      "1  book launch today for in cmb at 4pm followed b...  human  female\n",
      "2  on thursday at 10.30 am in edinburgh, roundtab...  human  female\n",
      "3  new italian law sho ens prison sentences by 3 ...  human  female\n",
      "4  shit bags had ht ft 35/1 brian kerr or mark la...  human    male\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# regular expression to detect emojis\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\"  \n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "    \"\\U0001F780-\\U0001F7FF\"  # geometric shapes\n",
    "    \"\\U0001F800-\\U0001F8FF\"  # supplemental arrows\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n",
    "    \"\\U0001FA00-\\U0001FA6F\"  # chess symbols\n",
    "    \"\\U0001FA70-\\U0001FAFF\"  # symbols and pictographs\n",
    "    \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "    \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "    \"]\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "# load the CSV data into a pandas DataFrame for the training set\n",
    "df = pd.read_csv('training.csv')\n",
    "\n",
    "# function to clean tweets: replace mentions, URLs, hashtags, remove multiple spaces, and lowercase\n",
    "def remove_clean_tweet(tweet):\n",
    "    tweet = tweet.lower()  # convert tweet to lowercase\n",
    "    # remove hashtags \n",
    "    tweet = re.sub(r'#\\S+', ' ', tweet)\n",
    "    # remove URLs\n",
    "    tweet = re.sub(r'http[s]?://\\S+', ' ', tweet)  # for URLs starting with http:// or https://\n",
    "    tweet = re.sub(r'www\\S+', ' ', tweet)  # for URLs starting with www\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'@\\S+', ' ', tweet)  # for mentions like @username\n",
    "    # remove emojis\n",
    "    tweet = re.sub(emoji_pattern, ' ', tweet)\n",
    "    # remove retweets\n",
    "    tweet = re.sub('rt', ' ', tweet)\n",
    "    # remove extra spaces and leading/trailing spaces\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()  # replace multiple spaces with a single space\n",
    "    return tweet\n",
    "\n",
    "# apply the cleaning function to the 'tweet' column\n",
    "df['tweet'] = df['tweet'].apply(remove_clean_tweet)\n",
    "\n",
    "# display the cleaned dataframe\n",
    "print(df.head())\n",
    "\n",
    "# save the cleaned DataFrame back to a new CSV file\n",
    "df.to_csv('removed_training.csv', index=False)\n",
    "\n",
    "# load the CSV data into a pandas DataFrame for the test set\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# apply the cleaning function to the 'tweet' column\n",
    "df_test['tweet'] = df_test['tweet'].apply(remove_clean_tweet)\n",
    "\n",
    "# display the cleaned dataframe\n",
    "print(df_test.head())\n",
    "\n",
    "# save the cleaned DataFrame back to a new CSV file\n",
    "df_test.to_csv('removed_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b23c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[373  39]\n",
      " [ 15 397]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.96      0.91      0.93       412\n",
      "       human       0.91      0.96      0.94       412\n",
      "\n",
      "    accuracy                           0.93       824\n",
      "   macro avg       0.94      0.93      0.93       824\n",
      "weighted avg       0.94      0.93      0.93       824\n",
      "\n",
      "Accuracy Score: 0.93\n",
      "\n",
      "Test Results:\n",
      "Confusion Matrix:\n",
      " [[1111  209]\n",
      " [  85 1235]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.93      0.84      0.88      1320\n",
      "       human       0.86      0.94      0.89      1320\n",
      "\n",
      "    accuracy                           0.89      2640\n",
      "   macro avg       0.89      0.89      0.89      2640\n",
      "weighted avg       0.89      0.89      0.89      2640\n",
      "\n",
      "Accuracy Score: 0.89\n",
      "\n",
      "Top 10 Important Features:\n",
      "          Feature  Importance\n",
      "887         today    0.185196\n",
      "873        thanks    0.149339\n",
      "467          just    0.118960\n",
      "872         thank    0.076580\n",
      "959          week    0.039745\n",
      "450  introduction    0.038720\n",
      "890       tonight    0.029781\n",
      "994          year    0.028727\n",
      "331          fuck    0.027594\n",
      "501          life    0.018589\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "# extract tweet text and labels\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# convert tweets to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # limit to top 1000 features for efficiency\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "y = labels\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)\n",
    "\n",
    "# train Gradient Boosting Classifier\n",
    "clf = GradientBoostingClassifier(random_state=39)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate on validation data\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Validation Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(y_val, y_pred), 2))\n",
    "\n",
    "# load test set\n",
    "df_test = pd.read_csv('removed_test.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column of test set\n",
    "df_test['tweet'] = df_test['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "tweets_test = df_test['tweet']\n",
    "labels_test = df_test['label']\n",
    "\n",
    "# convert test tweets to TF-IDF features\n",
    "X_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate on test data\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels_test, y_test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(labels_test, y_test_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(labels_test, y_test_pred), 2))\n",
    "\n",
    "# add predictions to the test DataFrame and save\n",
    "df_test['predicted_label'] = y_test_pred\n",
    "df_test.to_csv('removed_prediction_test.csv', index=False)\n",
    "\n",
    "# feature importance analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# create a DataFrame for feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# display top 10 features by importance\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# save top features to a CSV file\n",
    "importance_df.to_csv('top_features_removed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e43de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[364  48]\n",
      " [ 11 401]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.97      0.88      0.93       412\n",
      "       human       0.89      0.97      0.93       412\n",
      "\n",
      "    accuracy                           0.93       824\n",
      "   macro avg       0.93      0.93      0.93       824\n",
      "weighted avg       0.93      0.93      0.93       824\n",
      "\n",
      "Accuracy Score: 0.93\n",
      "\n",
      "Test Results:\n",
      "Confusion Matrix:\n",
      " [[1074  246]\n",
      " [  59 1261]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.95      0.81      0.88      1320\n",
      "       human       0.84      0.96      0.89      1320\n",
      "\n",
      "    accuracy                           0.88      2640\n",
      "   macro avg       0.89      0.88      0.88      2640\n",
      "weighted avg       0.89      0.88      0.88      2640\n",
      "\n",
      "Accuracy Score: 0.88\n",
      "\n",
      "Top 10 Important Features:\n",
      "     Feature  Importance\n",
      "467     just    0.033561\n",
      "959     week    0.030327\n",
      "873   thanks    0.026300\n",
      "890  tonight    0.023861\n",
      "872    thank    0.022626\n",
      "887    today    0.020864\n",
      "503     like    0.019309\n",
      "994     year    0.016625\n",
      "960  weekend    0.013606\n",
      "476     know    0.012618\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "# extract tweet text and labels\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# convert tweets to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # limit to top 1000 features for efficiency\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "y = labels\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)\n",
    "\n",
    "# train Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=39, n_estimators=100)  # use 100 trees\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate on validation data\n",
    "y_pred = clf.predict(X_val)\n",
    "print(\"Validation Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(y_val, y_pred), 2))\n",
    "\n",
    "# save validation predictions\n",
    "df_validation = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "df_validation.to_csv('validation_predictions_RF.csv', index=False)\n",
    "\n",
    "# load test set\n",
    "df_test = pd.read_csv('removed_test.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column of test set\n",
    "df_test['tweet'] = df_test['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "tweets_test = df_test['tweet']\n",
    "labels_test = df_test['label']\n",
    "\n",
    "# convert test tweets to TF-IDF features\n",
    "X_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate on test data\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels_test, y_test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(labels_test, y_test_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(labels_test, y_test_pred), 2))\n",
    "\n",
    "# add predictions to the test DataFrame and save\n",
    "df_test['predicted_label'] = y_test_pred\n",
    "df_test.to_csv('removed_prediction_test_RF.csv', index=False)\n",
    "\n",
    "# feature Importance Analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# create a DataFrame for feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# display top 10 features by importance\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# save top features to a CSV file\n",
    "importance_df.to_csv('top_features_removed_RF.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e3dc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM...\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[389  23]\n",
      " [  8 404]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.98      0.94      0.96       412\n",
      "       human       0.95      0.98      0.96       412\n",
      "\n",
      "    accuracy                           0.96       824\n",
      "   macro avg       0.96      0.96      0.96       824\n",
      "weighted avg       0.96      0.96      0.96       824\n",
      "\n",
      "Accuracy Score: 0.96\n",
      "\n",
      "Test Results:\n",
      "Confusion Matrix:\n",
      " [[1116  204]\n",
      " [  78 1242]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.93      0.85      0.89      1320\n",
      "       human       0.86      0.94      0.90      1320\n",
      "\n",
      "    accuracy                           0.89      2640\n",
      "   macro avg       0.90      0.89      0.89      2640\n",
      "weighted avg       0.90      0.89      0.89      2640\n",
      "\n",
      "Accuracy Score: 0.89\n",
      "\n",
      "Top 10 Important Features:\n",
      "    Feature  Importance\n",
      "451     iot    2.730901\n",
      "799   small    2.339289\n",
      "887   today    2.124059\n",
      "120  canada    1.887506\n",
      "872   thank    1.837330\n",
      "934   video    1.812242\n",
      "600    nice    1.786512\n",
      "359     gop    1.718278\n",
      "992      xx    1.692146\n",
      "938    vote    1.683030\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# load training data\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "# extract tweet text and labels\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# convert tweets to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # limit to top 1000 features for efficiency\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "y = labels\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)\n",
    "\n",
    "# train SVM Classifier\n",
    "print(\"Training SVM...\")\n",
    "svm_clf = SVC(kernel='linear', random_state=39)  # linear kernel for interpretability\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate on validation data\n",
    "y_val_pred = svm_clf.predict(X_val)\n",
    "print(\"Validation Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(y_val, y_val_pred), 2))\n",
    "\n",
    "# save validation predictions\n",
    "df_validation = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_val_pred\n",
    "})\n",
    "df_validation.to_csv('validation_predictions_SVM.csv', index=False)\n",
    "\n",
    "# load test set\n",
    "df_test = pd.read_csv('removed_test.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column of test set\n",
    "df_test['tweet'] = df_test['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "tweets_test = df_test['tweet']\n",
    "labels_test = df_test['label']\n",
    "\n",
    "# convert test tweets to TF-IDF features\n",
    "X_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# evaluate on test data\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels_test, y_test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(labels_test, y_test_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(labels_test, y_test_pred), 2))\n",
    "\n",
    "# add predictions to the test DataFrame and save\n",
    "df_test['predicted_label'] = y_test_pred\n",
    "df_test.to_csv('removed_prediction_test_SVM.csv', index=False)\n",
    "\n",
    "# feature importance analysis\n",
    "svm_weights = np.abs(svm_clf.coef_.toarray()[0])  # extract absolute weights for SVM\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': svm_weights\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# display top 10 features by importance\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# save top features to a CSV file\n",
    "importance_df.to_csv('top_features_removed_SVM.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73244aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Validation Results:\n",
      "Confusion Matrix:\n",
      " [[382  30]\n",
      " [ 11 401]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.97      0.93      0.95       412\n",
      "       human       0.93      0.97      0.95       412\n",
      "\n",
      "    accuracy                           0.95       824\n",
      "   macro avg       0.95      0.95      0.95       824\n",
      "weighted avg       0.95      0.95      0.95       824\n",
      "\n",
      "Accuracy Score: 0.95\n",
      "\n",
      "Test Results:\n",
      "Confusion Matrix:\n",
      " [[1127  193]\n",
      " [  81 1239]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         bot       0.93      0.85      0.89      1320\n",
      "       human       0.87      0.94      0.90      1320\n",
      "\n",
      "    accuracy                           0.90      2640\n",
      "   macro avg       0.90      0.90      0.90      2640\n",
      "weighted avg       0.90      0.90      0.90      2640\n",
      "\n",
      "Accuracy Score: 0.9\n",
      "\n",
      "Top 10 Important Features:\n",
      "          Feature  Importance\n",
      "887         today    3.264989\n",
      "873        thanks    3.237364\n",
      "451           iot    3.014501\n",
      "467          just    2.902002\n",
      "872         thank    2.702731\n",
      "205           day    2.341842\n",
      "992            xx    2.337696\n",
      "417         https    2.259920\n",
      "979          work    2.117327\n",
      "450  introduction    2.117060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# load training data\n",
    "df = pd.read_csv('removed_training.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column\n",
    "df['tweet'] = df['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "# extract tweet text and labels\n",
    "tweets = df['tweet']\n",
    "labels = df['label']\n",
    "\n",
    "# convert tweets to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # limit to top 1000 features for efficiency\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "y = labels\n",
    "\n",
    "# split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39, stratify=y)\n",
    "\n",
    "# train Logistic Regression Classifier\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_clf = LogisticRegression(random_state=39, max_iter=1000)  # increased iterations for convergence\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate on validation data\n",
    "y_val_pred = lr_clf.predict(X_val)\n",
    "print(\"Validation Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(y_val, y_val_pred), 2))\n",
    "\n",
    "# save validation predictions\n",
    "df_validation = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_val_pred\n",
    "})\n",
    "df_validation.to_csv('validation_predictions_LR.csv', index=False)\n",
    "\n",
    "# load test set\n",
    "df_test = pd.read_csv('removed_test.csv')\n",
    "\n",
    "# check for and replace missing values in the 'tweet' column of test set\n",
    "df_test['tweet'] = df_test['tweet'].fillna('')  # replace NaN with empty string\n",
    "\n",
    "tweets_test = df_test['tweet']\n",
    "labels_test = df_test['label']\n",
    "\n",
    "# convert test tweets to TF-IDF features\n",
    "X_test = vectorizer.transform(tweets_test)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = lr_clf.predict(X_test)\n",
    "\n",
    "# evaluate on test data\n",
    "print(\"\\nTest Results:\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(labels_test, y_test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(labels_test, y_test_pred))\n",
    "print(\"Accuracy Score:\", round(accuracy_score(labels_test, y_test_pred), 2))\n",
    "\n",
    "# add predictions to the test DataFrame and save\n",
    "df_test['predicted_label'] = y_test_pred\n",
    "df_test.to_csv('removed_prediction_test_LR.csv', index=False)\n",
    "\n",
    "# feature importance analysis\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "lr_weights = abs(lr_clf.coef_[0])  # extract absolute weights for Logistic Regression\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': lr_weights\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# display top 10 features by importance\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# save top features to a CSV file\n",
    "importance_df.to_csv('top_features_removed_LR.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
